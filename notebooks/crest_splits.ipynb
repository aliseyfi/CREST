{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting CREST to popular formats\n",
    "In this notebook, we show how CREST can be formatted to popular and useful formats such as BRAT for Relation Annotation or TACRED, a popular datasert for relation extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "root_path = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.insert(0, root_path)\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from crest.utils import crest2tacred\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CREST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the CREST-formatted data\n",
    "df = pd.read_excel('../data/causal/crest.xlsx', index_col=[0])\n",
    "\n",
    "# check if split is nan, then set the split to train\n",
    "df.loc[np.isnan(df['split']),'split'] = 0\n",
    "\n",
    "# check if there's no more nan split value\n",
    "assert len(df.loc[np.isnan(df['split'])]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>span1</th>\n",
       "      <th>span2</th>\n",
       "      <th>signal</th>\n",
       "      <th>context</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>ann_file</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['tumor shrinkage']</td>\n",
       "      <td>['radiation therapy']</td>\n",
       "      <td>[]</td>\n",
       "      <td>The period of tumor shrinkage after radiation ...</td>\n",
       "      <td>{'span1': [[14, 29]], 'span2': [[36, 53]], 'si...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['Habitat degradation']</td>\n",
       "      <td>['stream channels']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Habitat degradation from within stream channel...</td>\n",
       "      <td>{'span1': [[0, 19]], 'span2': [[32, 47]], 'sig...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>['discomfort']</td>\n",
       "      <td>['traveling']</td>\n",
       "      <td>[]</td>\n",
       "      <td>Earplugs relieve the discomfort from traveling...</td>\n",
       "      <td>{'span1': [[21, 31]], 'span2': [[37, 46]], 'si...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>['daily terror']</td>\n",
       "      <td>['antipersonnel land mines']</td>\n",
       "      <td>[]</td>\n",
       "      <td>We continue to see progress toward a world fre...</td>\n",
       "      <td>{'span1': [[55, 67]], 'span2': [[71, 95]], 'si...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['segment']</td>\n",
       "      <td>['anecdotes']</td>\n",
       "      <td>[]</td>\n",
       "      <td>The Global Warming segment starts off with two...</td>\n",
       "      <td>{'span1': [[19, 26]], 'span2': [[53, 62]], 'si...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_id                    span1                         span2 signal  \\\n",
       "0           1      ['tumor shrinkage']         ['radiation therapy']     []   \n",
       "1           2  ['Habitat degradation']           ['stream channels']     []   \n",
       "2           3           ['discomfort']                 ['traveling']     []   \n",
       "3           4         ['daily terror']  ['antipersonnel land mines']     []   \n",
       "4           5              ['segment']                 ['anecdotes']     []   \n",
       "\n",
       "                                             context  \\\n",
       "0  The period of tumor shrinkage after radiation ...   \n",
       "1  Habitat degradation from within stream channel...   \n",
       "2  Earplugs relieve the discomfort from traveling...   \n",
       "3  We continue to see progress toward a world fre...   \n",
       "4  The Global Warming segment starts off with two...   \n",
       "\n",
       "                                                 idx  label  source ann_file  \\\n",
       "0  {'span1': [[14, 29]], 'span2': [[36, 53]], 'si...      2       1      NaN   \n",
       "1  {'span1': [[0, 19]], 'span2': [[32, 47]], 'sig...      0       1      NaN   \n",
       "2  {'span1': [[21, 31]], 'span2': [[37, 46]], 'si...      2       1      NaN   \n",
       "3  {'span1': [[55, 67]], 'span2': [[71, 95]], 'si...      2       1      NaN   \n",
       "4  {'span1': [[19, 26]], 'span2': [[53, 62]], 'si...      0       1      NaN   \n",
       "\n",
       "   split  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data splits\n",
    "In this section, we prepare the train and dev/test splits for experiments. Setup is in a way that we first separate positive and negative samples, then we split them by `frac_val` (e.g. %80/%20 for train and dev/test, respectively). Then we combine the train splits of positive and negative samples and also the dev/test splits of positiv and negative samples. The reason is that we want to make sure that train and dev/test are not dominated by one class.\n",
    "\n",
    "To include a data in this operation, it is enough to add the data source code in CREST to `source_codes` list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(original data) positive: 4273, negative: 2369\n",
      "train: 5978, dev: 664\n"
     ]
    }
   ],
   "source": [
    "# [1] selecting SemEval sub-data\n",
    "source_codes = [3, 4, 5, 6, 7]\n",
    "# source_codes = [1, 2]\n",
    "sem_df = df[df['source'].isin(source_codes)]\n",
    "\n",
    "# [1.1] getting number of causal (positive) and non-causal (negative) samples\n",
    "n_pos = len(sem_df[sem_df['label'].isin([1, 2])])\n",
    "n_neg = len(sem_df[sem_df['label'].isin([0])])\n",
    "\n",
    "print(\"(original data) positive: {}, negative: {}\".format(n_pos, n_neg))\n",
    "\n",
    "# args = {'frac_val': 0.8, 'n_neg': 3 * n_pos, 'n_pos': n_pos}\n",
    "args = {'frac_val': 0.9, 'n_neg': n_neg, 'n_pos': n_pos}\n",
    "\n",
    "# [2] train and dev splits\n",
    "# [2.1] non-causal train and dev\n",
    "sem_neg = sem_df.loc[sem_df['label'] == 0].sample(n=args['n_neg'], random_state=42)\n",
    "neg_train = sem_neg.apply(lambda x: x.sample(frac=args['frac_val'], random_state=42))\n",
    "neg_dev = sem_neg.drop(neg_train.index)\n",
    "\n",
    "# [2.2] causal train and dev\n",
    "sem_pos = sem_df[sem_df['label'].isin([1, 2])].sample(n=args['n_pos'], random_state=42)\n",
    "pos_train = sem_pos.apply(lambda x: x.sample(frac=args['frac_val'], random_state=42))\n",
    "pos_dev = sem_pos.drop(pos_train.index)\n",
    "\n",
    "# concatenating causal and non-causal samples \n",
    "train_frames = [neg_train, pos_train]\n",
    "dev_frames = [neg_dev, pos_dev]\n",
    "\n",
    "train_df = pd.concat(train_frames)\n",
    "dev_df = pd.concat(dev_frames)\n",
    "\n",
    "# shuffling samples\n",
    "train_df = train_df.sample(frac=1, random_state=42)\n",
    "dev_df = dev_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# changing the ternary classes to binary (1 and 2 are both causal)\n",
    "train_df.loc[train_df['label'] == 2, 'label'] = 1\n",
    "dev_df.loc[dev_df['label'] == 2, 'label'] = 1\n",
    "\n",
    "print(\"train: {}, dev: {}\".format(len(train_df), len(dev_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert CREST to TACRED\n",
    "TACRED is a well-known and popular dataset for relation extraction. TACRED has been used in many studies as a bechnmark for evaluating the performance of models for relation extraction. These models include but not limited to popular language models such as BERT. That is why we decided to include TACRED as one the formats that CREST can be converted to. You can find TACRED here: https://nlp.stanford.edu/projects/tacred/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': train_df, 'dev': dev_df}\n",
    "\n",
    "for key, value in splits.items():\n",
    "    data = crest2tacred(value, key, save_json=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking context overlaps between splits\n",
    "To make sure that there's no/least overlap between the context values in train and dev, we check the context similarity of these two splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== with overlap ====\n",
      "train: 5526, unique: 2381\n",
      "dev: 621, unique: 519\n",
      "\n",
      "==== without overlap ====\n",
      "train size: 3858\n"
     ]
    }
   ],
   "source": [
    "resolve_overlap = 'train'\n",
    "\n",
    "dev_path = '../data/causal/splits/dev.json'\n",
    "train_path = '../data/causal/splits/train.json'\n",
    "\n",
    "dev = pd.read_json(dev_path)\n",
    "train = pd.read_json(train_path)\n",
    "\n",
    "split_info = {'dev_data': dev, 'dev_path': dev_path, 'train_data': train, 'train_path': train_path}\n",
    "\n",
    "train_context = []\n",
    "dev_context = []\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    train_context.append(' '.join(row['token']))\n",
    "for index, row in dev.iterrows():\n",
    "    dev_context.append(' '.join(row['token']))\n",
    "\n",
    "print('==== with overlap ====')\n",
    "print('train: {}, unique: {}'.format(len(train_context), len(set(train_context))))\n",
    "print('dev: {}, unique: {}'.format(len(dev_context), len(set(dev_context))))\n",
    "\n",
    "## finding overlaps\n",
    "overlaps = list(set(train_context) & set(dev_context))\n",
    "\n",
    "records = []    \n",
    "for index, row in split_info[resolve_overlap + '_data'].iterrows():\n",
    "    if ' '.join(row['token']) not in overlaps:\n",
    "        records.append(row.to_dict())\n",
    "\n",
    "# removing the old split file\n",
    "os.remove(split_info[resolve_overlap + '_path'])\n",
    "\n",
    "# saving records into a JSON file\n",
    "with open(split_info[resolve_overlap + '_path'], 'w') as fout:\n",
    "    json.dump(records, fout)\n",
    "\n",
    "print('\\n==== without overlap ====')\n",
    "print('{} size: {}'.format(resolve_overlap, len(records)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels: {1: 2957, 0: 901}\n",
      "dev labels: {1: 404, 0: 217}\n",
      "micro-f1: 0.6505636070853462\n",
      "macro-f1: 0.39414634146341465\n",
      "weighted-f1: 0.7882926829268293\n",
      "[[  0   0]\n",
      " [217 404]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.65      0.79       621\n",
      "\n",
      "    accuracy                           0.65       621\n",
      "   macro avg       0.50      0.33      0.39       621\n",
      "weighted avg       1.00      0.65      0.79       621\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phosseini/anaconda3/envs/crest-env/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dev = pd.read_json('../data/causal/splits/dev.json')\n",
    "train = pd.read_json('../data/causal/splits/train.json')\n",
    "\n",
    "labels = []\n",
    "preds = []\n",
    "train_labels = {}\n",
    "dev_labels = {}\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    if row['relation'] in train_labels:\n",
    "        train_labels[row['relation']] += 1\n",
    "    else:\n",
    "        train_labels[row['relation']] = 1\n",
    "\n",
    "for index, row in dev.iterrows():\n",
    "    labels.append(row['relation'])\n",
    "\n",
    "c = Counter(labels)\n",
    "major_label, count = c.most_common()[0]\n",
    "    \n",
    "for i in range(len(labels)):\n",
    "    if labels[i] in dev_labels:\n",
    "        dev_labels[labels[i]] += 1\n",
    "    else:\n",
    "        dev_labels[labels[i]] = 1\n",
    "                \n",
    "    preds.append(major_label)\n",
    "\n",
    "print(\"train labels: {}\".format(train_labels))\n",
    "print(\"dev labels: {}\".format(dev_labels))\n",
    "\n",
    "print(\"micro-f1: {}\".format(f1_score(preds, labels, average='micro')))\n",
    "print(\"macro-f1: {}\".format(f1_score(preds, labels, average='macro')))\n",
    "print(\"weighted-f1: {}\".format(f1_score(preds, labels, average='weighted')))\n",
    "\n",
    "print(confusion_matrix(preds, labels))\n",
    "print(classification_report(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crest-env] *",
   "language": "python",
   "name": "conda-env-crest-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
